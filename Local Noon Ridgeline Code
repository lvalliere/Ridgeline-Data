import networkx as nx
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib as plt
import matplotlib.pyplot as plt  
from ChemicalCase import ChemicalCase
import csv

df = pd.read_csv("gckpp_EdgeList.csv")
B = nx.DiGraph()
B.add_edges_from([(df["from"][i],df["to"][i]) for i in range(0,len(df["to"]))])
# All code below follows the same exact syntax as the Amazon, meaning all code explanation would be redundant.
# However, the only difference is the file read in, which is the Beijing dataset.

filename = "Amazon_L1_20180702_1600.txt"
surface = ChemicalCase(filename)
                
for i in range(1,914):
    # For all edges coming in and out of a reaction node, set to half the rxn timescale 
    for u,v in B.in_edges("R"+str(i)):
        B[u][v]['timescale'] = 0.5 / (surface.reaction_rates[i-1] + 1e-20) # 1e-20 to avoid divide by 0
    for u,v in B.out_edges("R"+str(i)):
        B[u][v]['timescale'] = 0.5 / (surface.reaction_rates[i-1] + 1e-20)


rxn_node_list = ['R' + str(i) for i in range(1,914)]
spc_node_list = [node for node in list(B.nodes) if node not in rxn_node_list]

shortest_paths = [] 
shortest_timescales = []
count = 0
for source_node in spc_node_list: # Ensure the source node is not the same as the target node
    for target_node in spc_node_list: # Check if there exists a path from the source node to the target node
        if source_node != target_node and nx.has_path(B, source_node, target_node):
            count += 1
            print('\rCalculating shortest path #', str(count), end='')
            path = nx.shortest_path(B, source=source_node, target=target_node,weight='timescale') 
            # similar ideas to Floyd-Warshall (getting all shortest paths from all source nodes)
            timescale = nx.path_weight(B, path,weight='timescale')
            shortest_paths.append(path)
            shortest_timescales.append(timescale)

    
df = pd.DataFrame(shortest_timescales)
df.to_csv('/Users/lucasvalliere/Desktop/Atmospheric Chemistry & Earth Data Science Work/Ridgeline Data/Amazon_LocalNoonTimescales.csv', index=False)

sns.kdeplot(shortest_timescales, log_scale=True, color='orange')
plt.title("Amazon Local Noon Shortest Paths")
plt.xlabel("Chemical Pathway Time Scale [seconds/molec/cc]")
plt.ylabel('Density')
plt.show()


-------



import numpy as np
import networkx as nx
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
sns.set_theme(style="white", rc={"axes.facecolor": (0, 0, 0, 0)})

# df = pd.read_csv("GCv14_Edgelist.csv")
df = pd.read_csv("gckpp_EdgeList.csv")
# Create DiGraph
B = nx.DiGraph()
# Add edges. Note CSV file had an extra space after the comma
B.add_edges_from([(df["from"][i],df["to"][i]) for i in range(0,len(df["to"]))])

# obtaining species and converting to list from SPC_NAMES.txt
my_file = open('Amazon_L1_20180702_1600.txt', "rt")
data = my_file.read()
species = data.split()
my_file.close()

reactions = ['R' + str(i) for i in range(1,914)]

spc_list = species

# Reading in stored cycles from csv -- CAN ADD YOUR PATHS DATA FILE HERE 
cyclesstored = open('Amazon_LocalNoonTimescales.csv','r') # <-- fill with your paths data here 
data = cyclesstored.read()
lines = data.split('\n')
cyclesstored.close()

lines.pop(0)
lines.pop(-1)

cycleslist = []
for i in range(0, len(lines)): 
    temp = lines[i].split(',')
    cycleslist.append(temp)     # KEEPS ROWS NOT EQUAL TO 0
    
totalcycles = cycleslist

# list to store the distribution data 
timescalelin = []

# list to store the repeat location names corresponding to the number of timescale values for that location
tiles = []

# location names as they are in the file names, in order of how we want them to show up on graph
locations = [['AtlanticOcean', '1500'], ['IndianOcean', '0600'], ['PacificOcean', '1900'], ['Graciosa', '1200'], ['CapeGrim', '0200'], ['ElDjouf', '1200'], ['Amazon', '1600'], ['Borneo', '0400'], ['Congo', '1100'], ['Ozarks', '1700'], ['Beijing', '0400'], ['Kinshasa', '1100'], ['LosAngeles', '1900'], ['Paris', '1000'], ['Utqiagvik', '2000'], ['McMurdo', '0000']]
# locations = [['Amazon', '1600']]
# location names as how we want them to appear on the graph
graphlocations = ['Atlantic Ocean', 'Indian Ocean', 'Pacific Ocean', 'Graciosa', 'Kennaook', 'El Djouf', 'Amazon', 'Borneo', 'Congo', 'Ozarks', 'Beijing', 'Kinshasa', 'Los Angeles', 'Paris', 'Utqiagvik', 'McMurdo Station']
# filename = 'L1 Noon Samples/AtlanticOcean_L1_20180702_2000.txt'
for location in locations: 
    filename = '/Users/lucasvalliere/Desktop/Atmospheric Chemistry & Earth Data Science Work/Ridgeline Data/' + str(location[0]) + '_L1_20180702_' + str(location[1]) + '.txt'
    sample_file = open(filename, "rt")
    sampledata = sample_file.read()
    sample = sampledata.split('\n ')
    sample_file.close()
    rates = [] 
    for r in range(0, len(sample)):                                                                 # No need for this code?
        if sample[r][0] == 'A': 
            temp = sample[r].split()
            rates.append(temp[4])    
    for i in range(0, len(rates)):
        rates[i] = float(rates[i])
    # list to hold residence times for each cycle in totalcycles
    cyclest = []
    # # track = 0
    for i in range(0, len(totalcycles)):                                                             # No need for this code?
        rnums = []
        sub_cyclest = 0
        # if the cycle on current iteration starts with a reaction or species
        if totalcycles[i][0] in reactions:
            for j in range(0, len(totalcycles[i]), 2):
                rnums.append(totalcycles[i][j].replace('R', ''))
        else: 
            for j in range(1, len(totalcycles[i]), 2):
                rnums.append(totalcycles[i][j].replace('R', ''))
        # add time per reaction to get residence time for the full cycle on current iteration 
        for m in range(0, len(rnums)): 
            if rates[int(rnums[m])-1] == float(0):
                sub_cyclest += float('inf')
            else: 
                sub_cyclest += 1/rates[int(rnums[m])-1]
        cyclest.append([sub_cyclest, totalcycles[i]])
    # # to save a copy of the residence times list before using the original cyclest one in sorting cycles
    # cycleststore = []
    # for i in range(0, len(cyclest)):
    #     cycleststore.append(cyclest[i])
    # # for the histogram
    # # removing the residence times that are 'inf' in order to be able to calculate bin size, noting that inf would skew it higher
    cyclest_noninf = []
    for i in range(0, len(cyclest)):
        if cyclest[i][0] != float('inf'): 
            cyclest_noninf.append([cyclest[i][0], cyclest[i][1]])
    cyclest_noninf.sort()
    # do we want to store the version of cyclest_noninf with both the timescale and their corresponding cycle? / helps with analysis? 
    times = []
    for i in range(0, len(cyclest_noninf)): 
        times.append(float(cyclest_noninf[i][0]))
    timescalelin += times
    # getting index of current iteration's location to put into tiles list
    index = locations.index(location)
    tiles += len(times) * [graphlocations[index]]
    
timescalelin = np.array(timescalelin)
timescale = np.log10(timescalelin)
df = pd.DataFrame(dict(timescale=timescale, tiles=tiles))
# m = df.tiles.map(ord)
# df["timescale"] += m

# Initialize the FacetGrid object
# pal = sns.cubehelix_palette(3, rot=-.25, light=.7)
# pal = sns.color_palette(palette='magma_r', n_colors=3)
pal = sns.cubehelix_palette(16, rot=-.25, light=.7)
pal[15] = pal[0]
pal[14] = pal[1]
pal[0] = pal[1] = pal[2] = pal[3] = pal[4] = [0.2154516905067, 0.295, 0.5078367867510504]
pal[5] = [0.78, 0.51, 0.1892045842]
pal[6] = pal[7] = pal[8] = pal[9] = [0.12071162840208301, 0.4357915995132193, 0.2463679091477368]
pal[10] = pal[11] = pal[12] = pal[13] = [0.7, 0.7, 0.65]

# you can play around with color schemes here 
# pal[0] = pal[1] = pal[2] = pal[3] = pal[4] = [0.6, 0.15, 0.6]
# pal[5] = [0.8, 0.18, 0.6]
# pal[6] = pal[7] = pal[8] = pal[9] = [0.99, 0.55, 0.2]
# pal[10] = pal[11] = pal[12] = pal[13] = [0.99, 0.65, 0.2]
# pal[14] = [0.99, 0.8, 0.2]
# pal[15] = [0.95, 0.83, 0.3]

# pal[13] = [0.78, 0.51, 0.1892045842]
tiles = sns.FacetGrid(df, row="tiles", hue="tiles", aspect=15, height=.5, palette=pal)

# Draw the densities 
tiles.map(sns.kdeplot, "timescale",
      bw_adjust=.5, clip_on=False,
      fill=True, alpha=1, linewidth=1.5)
tiles.map(sns.kdeplot, "timescale", clip_on=False, color="w", lw=2, bw_adjust=.5)

# passing color=None to refline() uses the hue mapping
tiles.refline(y=0, linewidth=2, linestyle="-", color=None, clip_on=False)


# Define and use a simple function to label the plot in axes coordinates
def label(x, color, label):
    ax = plt.gca()
    ax.text(0, .2, label, fontweight="bold", color=color,
            ha="left", va="center", transform=ax.transAxes)
    # ax.canvas.draw()
    # plt.tight_layout()

tiles.map(label, "timescale")

# Set the subplots to overlap
tiles.figure.subplots_adjust(hspace=-0.15)

# Remove axes details that don't play well with overlap
tiles.set_titles("")
tiles.set(yticks=[], ylabel="")
tiles.despine(bottom=True, left=True)

# plt.savefig('Noon Plots/Timescale Distributions/L1Paths_orSomeOtherFileName.pdf', bbox_inches="tight")
